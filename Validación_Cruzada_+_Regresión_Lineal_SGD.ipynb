{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Validación Cruzada + Regresión Lineal SGD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCq0RFmd7FW1eXwHnf8O9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aboltCD/aboltCD/blob/main/Validaci%C3%B3n_Cruzada_%2B_Regresi%C3%B3n_Lineal_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t657Bi9wRCp6"
      },
      "source": [
        "## Validación Cruzada\n",
        "\n",
        "En este notebook haremos predicciones sobre precios de casas en Boston utilizando regresiones lineales y el optimizador SGD.\n",
        "\n",
        "Sin embargo, el plato fuerte esta vez es el uso de validación cruzada para estabilizar los resultados al disminuir los sesgos de seleccion de datos e hiperparámetros.\n",
        "\n",
        "Comenzamos por importar las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLIgjUitRRj7"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO_Ivrgcc71P"
      },
      "source": [
        "Ahora cargamos el dataset de Boston Housing, lo separamos en Features y Target, lo escalamos y dividimos en entrenamiento y prueba (en este caso, utilizamos un 15% de los datos como prueba):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEquSt-KdBV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568b3df2-fef7-4f27-a6fc-ebd013e62b96"
      },
      "source": [
        "boston = load_boston()\n",
        "#X, y = boston.data, boston.target\n",
        "\n",
        "#print(boston.DESCR)\n",
        "\n",
        "df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "df[\"MEDV\"] = boston.target\n",
        "\n",
        "#Feature Matrix (todos los atributos)\n",
        "X = df.drop(\"MEDV\",1)  \n",
        "\n",
        "#Feature Matrix (sólo la cantidad de habitaciones)\n",
        "#X = df[[\"RM\"]] \n",
        "\n",
        "#Target Variable\n",
        "y = df[[\"MEDV\"]]         \n",
        "\n",
        "df.head()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.10, random_state = 1)\n",
        "\n",
        "scalerX = StandardScaler().fit(X_train)\n",
        "scalery = StandardScaler().fit(y_train)\n",
        "\n",
        "X_train = scalerX.transform(X_train)\n",
        "y_train = scalery.transform(y_train)\n",
        "X_test = scalerX.transform(X_test)\n",
        "y_test = scalery.transform(y_test)\n",
        "\n",
        "y_train.shape\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(455, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6A8koDmdfjH"
      },
      "source": [
        "Ahora entrenamos el modelo (ajustamos la regresión) y sacamos tres métricas de error: R^2, MSE (error cuadrado promedio), RMSE (raíz del error cuadrado promedio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySKxuIwndkry",
        "outputId": "e12ca995-bfc8-434d-f823-20cf26c4190c"
      },
      "source": [
        "sgdr = SGDRegressor(learning_rate='invscaling', eta0=0.1, n_iter_no_change = 5)\n",
        "\n",
        "#Métricas de validación cruzada para esa combinación de parámetros....\n",
        "scores = cross_val_score(sgdr, X_train, y_train.ravel(), cv=10)\n",
        "\n",
        "print(\"error promedio de validación cruzada\", scores.mean())\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error promedio de validación cruzada 0.6560687947237359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMf0jmFYerRQ"
      },
      "source": [
        "Sin embargo esta prueba y cálculo manual puede ser tedioso. Vamos a hacer algo más automático utilizando Grid Search Cross Validation para buscar el mejor parámetro en una \"Grilla\" de posibles candidatos (combinaciones de valores de parámetros) en donde cáda candidato será evaluado con validación cruzada: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Ldr15deq5u",
        "outputId": "6be0bc3e-7fee-4e04-e645-9d16261eb6ee"
      },
      "source": [
        "param_grid = [\n",
        "  {'eta0': [0.2, 0.15, 0.1, 0.05, 0.01], 'learning_rate': ['optimal', 'constant', 'invscaling', 'adaptive'], 'n_iter_no_change': [1, 5, 10, 20, 50, 100]}\n",
        " ]\n",
        "\n",
        "\n",
        "clf = GridSearchCV(estimator = SGDRegressor(), param_grid = param_grid, scoring = 'neg_mean_squared_error', cv=10) \n",
        "clf.fit(X_train, y_train.ravel())\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=SGDRegressor(alpha=0.0001, average=False,\n",
              "                                    early_stopping=False, epsilon=0.1,\n",
              "                                    eta0=0.01, fit_intercept=True,\n",
              "                                    l1_ratio=0.15, learning_rate='invscaling',\n",
              "                                    loss='squared_loss', max_iter=1000,\n",
              "                                    n_iter_no_change=5, penalty='l2',\n",
              "                                    power_t=0.25, random_state=None,\n",
              "                                    shuffle=True, tol=0.001,\n",
              "                                    validation_fraction=0.1, verbose=0,\n",
              "                                    warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid=[{'eta0': [0.2, 0.15, 0.1, 0.05, 0.01],\n",
              "                          'learning_rate': ['optimal', 'constant', 'invscaling',\n",
              "                                            'adaptive'],\n",
              "                          'n_iter_no_change': [1, 5, 10, 20, 50, 100]}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_squared_error', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImdfBZJNQt3K",
        "outputId": "c7820d51-020c-4c7c-8d0e-894f483643fb"
      },
      "source": [
        "\n",
        "\n",
        "print(\"Mejor score (error minimo cuadrado negativo): \", clf.best_score_)\n",
        "print(\"Mejores hiperparámetros: \", clf.best_params_)\n",
        "print(\"Mejor modelo: \", clf.best_estimator_)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor score (error minimo cuadrado):  -0.2843047852508456\n",
            "Mejores hiperparámetros:  {'eta0': 0.15, 'learning_rate': 'invscaling', 'n_iter_no_change': 100}\n",
            "Mejor modelo:  SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
            "             eta0=0.15, fit_intercept=True, l1_ratio=0.15,\n",
            "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
            "             n_iter_no_change=100, penalty='l2', power_t=0.25,\n",
            "             random_state=None, shuffle=True, tol=0.001,\n",
            "             validation_fraction=0.1, verbose=0, warm_start=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-6WB_X0In4n"
      },
      "source": [
        "Ahora podemos obtener los valores reales vs predecidos (no escalados) con la función StandardScaler.inverse_transform()"
      ]
    }
  ]
}